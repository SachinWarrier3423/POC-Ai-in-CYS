{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhedZ8hAAe2k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "41fcfae9-1623-490a-9b0d-d4ee0969670f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for drain3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Upload your HDFS log file (example: hdfs.log)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b711ab78-884f-4839-810a-781ae4d7adbc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b711ab78-884f-4839-810a-781ae4d7adbc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving HDFS_2k.log to HDFS_2k.log\n",
            "Uploaded file: HDFS_2k.log\n",
            "Saved clean_hdfs.csv\n",
            "Anomaly detection complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"4b5912e0-6c94-43ca-a394-0b567c01ee15\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"4b5912e0-6c94-43ca-a394-0b567c01ee15\")) {                    Plotly.newPlot(                        \"4b5912e0-6c94-43ca-a394-0b567c01ee15\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"template=%{x}\\u003cbr\\u003eoccurrences=%{y}\\u003cbr\\u003eanomaly=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"\\u003c*\\u003e \\u003c*\\u003e \\u003c*\\u003e INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: \\u003c*\\u003e is added to \\u003c*\\u003e size \\u003c*\\u003e\",\"\\u003c*\\u003e \\u003c*\\u003e \\u003c*\\u003e INFO dfs.DataNode$PacketResponder: PacketResponder \\u003c*\\u003e for block \\u003c*\\u003e terminating\",\"\\u003c*\\u003e \\u003c*\\u003e \\u003c*\\u003e INFO dfs.DataNode$PacketResponder: Received block \\u003c*\\u003e of size \\u003c*\\u003e from \\u003c*\\u003e\",\"\\u003c*\\u003e \\u003c*\\u003e \\u003c*\\u003e INFO dfs.DataNode$DataXceiver: Receiving block \\u003c*\\u003e src: \\u003c*\\u003e dest: \\u003c*\\u003e\",\"\\u003c*\\u003e \\u003c*\\u003e \\u003c*\\u003e INFO dfs.FSDataset: Deleting block \\u003c*\\u003e file \\u003c*\\u003e\",\"\\u003c*\\u003e \\u003c*\\u003e \\u003c*\\u003e INFO dfs.FSNamesystem: BLOCK* NameSystem.delete: \\u003c*\\u003e is added to invalidSet of \\u003c*\\u003e\",\"\\u003c*\\u003e \\u003c*\\u003e \\u003c*\\u003e INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: \\u003c*\\u003e \\u003c*\\u003e\",\"\\u003c*\\u003e \\u003c*\\u003e \\u003c*\\u003e WARN dfs.DataNode$DataXceiver: \\u003c*\\u003e exception while serving \\u003c*\\u003e to \\u003c*\\u003e\",\"\\u003c*\\u003e \\u003c*\\u003e \\u003c*\\u003e INFO dfs.DataNode$DataXceiver: \\u003c*\\u003e Served block \\u003c*\\u003e to \\u003c*\\u003e\",\"\\u003c*\\u003e \\u003c*\\u003e 13 INFO dfs.DataBlockScanner: Verification succeeded for \\u003c*\\u003e\",\"081110 \\u003c*\\u003e 19 INFO dfs.FSNamesystem: BLOCK* ask \\u003c*\\u003e to delete \\u003c*\\u003e\",\"081111 \\u003c*\\u003e \\u003c*\\u003e INFO dfs.DataNode$DataXceiver: Received block \\u003c*\\u003e src: \\u003c*\\u003e dest: \\u003c*\\u003e of size 67108864\",\"081110 211541 18 INFO dfs.DataNode: 10.250.15.198:50010 Starting thread to transfer block blk_4292382298896622412 to 10.250.15.240:50010\",\"081111 065254 19 INFO dfs.FSNamesystem: BLOCK* ask 10.250.17.177:50010 to delete blk_-8570780307468499817 blk_-9122557405432088649 blk_-4393063808227796056 blk_8767569714374844347 blk_7079754042611867581 blk_7608961006114219538 blk_-5017273584996436939 blk_-6537833125980536955 blk_7610838808763810123 blk_3300803097775546532 blk_-5120750586032922592 blk_1577274266662884430 blk_765879159867598347 blk_-9076085976403711202 blk_-3198963348573340497 blk_-4645750029177277209 blk_-5136142986912961316 blk_5677959846373741243 blk_2107477892986152528 blk_-4235116161537008844 blk_6082535783543982566 blk_-4809870147222033236 blk_8818706925296961012 blk_-5203577173046267127 blk_189089569009261656 blk_446299976487589160 blk_-3916247521166632303 blk_-3324962406687427922 blk_-1807424528783081572 blk_-6858401049333055963 blk_6036564204960295926 blk_-8140723044408248078 blk_-3800132731140204959 blk_1716344083117307767 blk_-5194808114606613364 blk_-5473871016976323232 blk_2920934363167004552 blk_8736689095894369097 blk_-7642734632751940776 blk_3408482260833769309 blk_118013751374560901 blk_7963891081239759520 blk_3813114133944383323 blk_3042818489384932576 blk_-4570173726231458270 blk_-1564644006975920581 blk_338095650783321996 blk_3150135312641203550 blk_4285859645577726288 blk_3438772130782939627 blk_2634772258588877972 blk_-6795664812575964130 blk_3923069610304693233 blk_-1782996202120067721 blk_2004418049430157212 blk_1932147224007687756 blk_-582901062969027153 blk_5072240701440032119 blk_-7919006477393039068 blk_-7318022361288598312 blk_-6974693594143537436 blk_-5435767047126325206 blk_-5805500288959332434 blk_-7109885589081848850 blk_2161580591957523893 blk_7240227881194993860 blk_-8298405680648445349 blk_-4253026248821272215 blk_8377661448601579317 blk_8029153852899017155 blk_-8754388319080705916 blk_-7844092300527332901 blk_710178463364063355 blk_-5136849989188547884 blk_8393887138377503163 blk_-6950176077776664217 blk_-6488701068659548195 blk_2537458728254532453 blk_364441107933628577 blk_6207861897580168557 blk_8814943807366894581 blk_-4150682644311695471 blk_9174833667156726933 blk_649427218152856001 blk_-7403541028238011236 blk_-334982586592048773 blk_61908781908925992 blk_6385574357371832424 blk_-66376131060945541 blk_1372596948297458670 blk_-3389135155401857220 blk_-6035411221441929663 blk_-5127580069634421247 blk_-5685246533892022418 blk_4977937528993040451 blk_5680538862600094527 blk_-8378747462487962732 blk_425101290285860876 blk_6306622708327890839 blk_-1067866602168873257\",\"081111 065303 19 INFO dfs.FSNamesystem: BLOCK* ask 10.250.10.213:50010 to delete blk_4029139044660806713 blk_-5471189807977280544 blk_6708643067868168687 blk_-500678958150296008 blk_-8597840983621849778 blk_-3610057702150392748 blk_-1709606535283888232 blk_-4154362211643572668 blk_-8892080524136798472 blk_5356427838869009345 blk_-6987238639050161133 blk_-5215128860160823363 blk_7186692462976470823 blk_-6538449588297475521 blk_-2165930080589343952 blk_-5524899010031625427 blk_6384439316405471171 blk_-2965258329365213675 blk_118950937507976810 blk_-1717088081766373300 blk_-3911466865418055820 blk_1237334407720045724 blk_-760015977981369567 blk_-6802007379650646616 blk_-7667535133893574689 blk_6865645438678864855 blk_4633996820313194570 blk_7225301266481603731 blk_-4930257130609958866 blk_-4124845864570823487 blk_4927011145115127531 blk_7234346856930822716 blk_7159969052744592746 blk_1296823600557793869 blk_2209319141644287774 blk_-622218131799806364 blk_-8154516246083521409 blk_4466433199471909449 blk_8406894133999850666 blk_991075908349619367 blk_-2081474832657208733 blk_-5573393775847919985 blk_2004177185950968695 blk_4041319486058127641 blk_6449230045010995668 blk_5978265573904271474 blk_-4813738732036414715 blk_4389340532803855247 blk_-857151863616763327 blk_-7200136644339435027 blk_-1454962873426270839 blk_-5012294311590635938 blk_7112727670634942639 blk_3335012758760643328 blk_3382627815322561484 blk_825124020036421636 blk_-8040559034239258688 blk_-5415591001139074826 blk_-1052513063506891954 blk_-1155882018729560343 blk_-5679835604685169040 blk_-4498808851217768984 blk_8345415947062862337 blk_8521655806854586696 blk_7602939593939794410 blk_-4833650023923869528 blk_7237730029042141635 blk_2860897425785746911 blk_-1937193099911148343 blk_5740615689780260922 blk_963252337613423037 blk_5537011318013544619 blk_2626057344048606017 blk_8296499240199635880 blk_7211071078501521087 blk_8823112510768971040 blk_-3366974935992288326 blk_-2947778702643296262 blk_7693891282153136044 blk_4644812717442758529 blk_-5724970555730638200 blk_-3039294462945223064 blk_-1729755380346651221 blk_-6448673813272428418 blk_-7724282460846954976 blk_2698691234887375588 blk_-4043525878322523713 blk_-5195120009388265 blk_8879208244602324204 blk_-5784376901556131897 blk_-5201149273969117873 blk_5253889604362640423 blk_7067050654303940677 blk_8992626816092659826 blk_-488462739843441981 blk_8543991617360374935 blk_1943146154560599630 blk_-9194660123773136535 blk_3351984198891394382 blk_-6759123807563555545\",\"081111 080934 19 INFO dfs.FSNamesystem: BLOCK* ask 10.250.14.38:50010 to replicate blk_-7571492020523929240 to datanode(s) 10.251.122.38:50010\",\"081111 091733 19 INFO dfs.FSNamesystem: BLOCK* ask 10.251.126.5:50010 to delete blk_-9016567407076718172 blk_-8695715290502978219 blk_-7168328752988473716 blk_-4355192005224403537 blk_-3757501769775889193 blk_-154600013573668394 blk_167132135416677587 blk_2654596473569751784 blk_5202581916713319258\"],\"xaxis\":\"x\",\"y\":[314,311,292,292,263,224,115,80,80,20,2,2,1,1,1,1,1],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"template\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"occurrences\"}},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"anomaly\"}},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Top 20 HDFS Templates with Anomaly Detection\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('4b5912e0-6c94-43ca-a394-0b567c01ee15');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SINGLE DATASET: HDFS LOG ANOMALY DETECTION\n",
        "# Drain3 log parser + Isolation Forest + Dashboard\n",
        "# ============================================================\n",
        "\n",
        "# --------------------------\n",
        "# STEP 0: Install Libraries\n",
        "# --------------------------\n",
        "!pip install drain3 pandas scikit-learn plotly --quiet\n",
        "\n",
        "# --------------------------\n",
        "# STEP 1: Import Libraries\n",
        "# --------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from drain3 import TemplateMiner\n",
        "from drain3.file_persistence import FilePersistence\n",
        "from drain3.template_miner_config import TemplateMinerConfig\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import plotly.express as px\n",
        "from google.colab import files\n",
        "\n",
        "# --------------------------\n",
        "# STEP 2: Upload HDFS Log File\n",
        "# --------------------------\n",
        "print(\"Upload your HDFS log file (example: hdfs.log)\")\n",
        "uploaded = files.upload()\n",
        "hdfs_file = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {hdfs_file}\")\n",
        "\n",
        "# --------------------------\n",
        "# STEP 3: Parse Logs with Drain3\n",
        "# --------------------------\n",
        "persistence = FilePersistence(\"drain3_state.bin\")\n",
        "config = TemplateMinerConfig()\n",
        "template_miner = TemplateMiner(persistence, config)\n",
        "\n",
        "with open(hdfs_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        template_miner.add_log_message(line.strip())\n",
        "\n",
        "# --------------------------\n",
        "# STEP 4: Convert Logs to Structured DataFrame\n",
        "# --------------------------\n",
        "logs = []\n",
        "for cluster in template_miner.drain.clusters:\n",
        "    logs.append({\n",
        "        'template_id': cluster.cluster_id,\n",
        "        'template': cluster.get_template(),\n",
        "        'occurrences': cluster.size\n",
        "    })\n",
        "\n",
        "hdfs_df = pd.DataFrame(logs)\n",
        "\n",
        "# Add a dummy label (0=normal, 1=anomaly) – for demonstration\n",
        "# In a real scenario, you would have labeled data\n",
        "hdfs_df['label'] = 0 # Assuming all are normal for this example\n",
        "\n",
        "# Scale template_id for ML\n",
        "scaler = StandardScaler()\n",
        "hdfs_df['template_id_scaled'] = scaler.fit_transform(hdfs_df[['template_id']])\n",
        "\n",
        "# Save structured CSV\n",
        "hdfs_df.to_csv(\"clean_hdfs.csv\", index=False)\n",
        "print(\"Saved clean_hdfs.csv\")\n",
        "\n",
        "# --------------------------\n",
        "# STEP 5: Anomaly Detection using Isolation Forest\n",
        "# --------------------------\n",
        "# Prepare data for Isolation Forest - using occurrences and scaled template ID\n",
        "features = hdfs_df[['occurrences', 'template_id_scaled']]\n",
        "iso = IsolationForest(contamination=0.01, random_state=42)\n",
        "hdfs_df['anomaly'] = iso.fit_predict(features)\n",
        "print(\"Anomaly detection complete!\")\n",
        "\n",
        "# Convert anomaly prediction to 0 (normal) and 1 (anomaly)\n",
        "hdfs_df['anomaly'] = hdfs_df['anomaly'].apply(lambda x: 1 if x == -1 else 0)\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# STEP 6: Dashboard Visualization\n",
        "# --------------------------\n",
        "# Top 20 templates\n",
        "fig = px.bar(hdfs_df.sort_values('occurrences', ascending=False).head(20),\n",
        "             x='template', y='occurrences', color='anomaly',\n",
        "             title=\"Top 20 HDFS Templates with Anomaly Detection\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4e0eaee4",
        "outputId": "03e0caba-26a3-44ef-f815-1c9db98374b1"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('clean_hdfs.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9438529-6298-48c3-8986-83a0053e0385\", \"clean_hdfs.csv\", 7117)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "5fed2aac",
        "outputId": "8e3b4c07-5be8-4eee-bd50-9b5284b3ca15"
      },
      "source": [
        "print(\"Characteristics of Anomalous Logs:\")\n",
        "display(anomalous_logs.groupby(['template_id', 'template']).agg({'occurrences': 'sum', 'anomaly': 'count'}).rename(columns={'anomaly': 'num_anomalous_occurrences'}).sort_values(by='occurrences', ascending=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characteristics of Anomalous Logs:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'anomalous_logs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3520471833.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Characteristics of Anomalous Logs:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomalous_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'template_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'template'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'occurrences'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'anomaly'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'anomaly'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'num_anomalous_occurrences'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'occurrences'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'anomalous_logs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "995c1f8d"
      },
      "source": [
        "anomalous_logs = hdfs_df[hdfs_df['anomaly'] == 1]\n",
        "display(anomalous_logs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efb42067"
      },
      "source": [
        "fig = px.scatter(hdfs_df, x='occurrences', y='template_id_scaled', color='anomaly',\n",
        "                 title='Anomaly Detection Results (Scatter Plot)',\n",
        "                 labels={'occurrences': 'Occurrences', 'template_id_scaled': 'Scaled Template ID'},\n",
        "                 hover_data=['template'])\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc5b6afc"
      },
      "source": [
        "hdfs_df.to_csv(\"hdfs_anomalies.csv\", index=False)\n",
        "print(\"Saved hdfs_anomalies.csv with anomaly labels.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c24c76c5"
      },
      "source": [
        "# Task\n",
        "Compare the original HDFS log file with the processed data in \"/content/clean_hdfs.csv\", describing the changes made during processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4aedb44"
      },
      "source": [
        "## Load the original log file\n",
        "\n",
        "### Subtask:\n",
        "Read the original HDFS log file (the one uploaded by the user) line by line.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54b4ad9b"
      },
      "source": [
        "**Reasoning**:\n",
        "Read the original HDFS log file line by line to inspect its content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44d0938f"
      },
      "source": [
        "# Read the original HDFS log file line by line\n",
        "original_logs = []\n",
        "with open(hdfs_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        original_logs.append(line.strip())\n",
        "\n",
        "# Display the first few lines of the original logs\n",
        "print(\"First 5 lines of the original HDFS log file:\")\n",
        "for i in range(min(5, len(original_logs))):\n",
        "    print(original_logs[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77a23db9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the variable `hdfs_file` was not defined in the current execution context. I need to re-execute the cell where `hdfs_file` was defined and the file was uploaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tIN7gf1kdXx"
      },
      "source": [
        "# --------------------------\n",
        "# STEP 0: Install Libraries\n",
        "# --------------------------\n",
        "!pip install drain3 pandas scikit-learn plotly --quiet\n",
        "\n",
        "# --------------------------\n",
        "# STEP 1: Import Libraries\n",
        "# --------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from drain3 import TemplateMiner\n",
        "from drain3.file_persistence import FilePersistence\n",
        "from drain3.template_miner_config import TemplateMinerConfig\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import plotly.express as px\n",
        "from google.colab import files\n",
        "\n",
        "# --------------------------\n",
        "# STEP 2: Upload HDFS Log File\n",
        "# --------------------------\n",
        "print(\"Upload your HDFS log file (example: hdfs.log)\")\n",
        "uploaded = files.upload()\n",
        "hdfs_file = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file: {hdfs_file}\")\n",
        "\n",
        "# --------------------------\n",
        "# STEP 3: Parse Logs with Drain3\n",
        "# --------------------------\n",
        "persistence = FilePersistence(\"drain3_state.bin\")\n",
        "config = TemplateMinerConfig()\n",
        "template_miner = TemplateMiner(persistence, config)\n",
        "\n",
        "with open(hdfs_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        template_miner.add_log_message(line.strip())\n",
        "\n",
        "# --------------------------\n",
        "# STEP 4: Convert Logs to Structured DataFrame\n",
        "# --------------------------\n",
        "logs = []\n",
        "for cluster in template_miner.drain.clusters:\n",
        "    logs.append({\n",
        "        'template_id': cluster.cluster_id,\n",
        "        'template': cluster.get_template(),\n",
        "        'occurrences': cluster.size\n",
        "    })\n",
        "\n",
        "hdfs_df = pd.DataFrame(logs)\n",
        "\n",
        "# Add a dummy label (0=normal, 1=anomaly) – for demonstration\n",
        "# In a real scenario, you would have labeled data\n",
        "hdfs_df['label'] = 0 # Assuming all are normal for this example\n",
        "\n",
        "# Scale template_id for ML\n",
        "scaler = StandardScaler()\n",
        "hdfs_df['template_id_scaled'] = scaler.fit_transform(hdfs_df[['template_id']])\n",
        "\n",
        "# Save structured CSV\n",
        "hdfs_df.to_csv(\"clean_hdfs.csv\", index=False)\n",
        "print(\"Saved clean_hdfs.csv\")\n",
        "\n",
        "# --------------------------\n",
        "# STEP 5: Anomaly Detection using Isolation Forest\n",
        "# --------------------------\n",
        "# Prepare data for Isolation Forest - using occurrences and scaled template ID\n",
        "features = hdfs_df[['occurrences', 'template_id_scaled']]\n",
        "iso = IsolationForest(contamination=0.01, random_state=42)\n",
        "hdfs_df['anomaly'] = iso.fit_predict(features)\n",
        "print(\"Anomaly detection complete!\")\n",
        "\n",
        "# Convert anomaly prediction to 0 (normal) and 1 (anomaly)\n",
        "hdfs_df['anomaly'] = hdfs_df['anomaly'].apply(lambda x: 1 if x == -1 else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "201ae355"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `hdfs_file` variable is defined after re-uploading the file, I can proceed with reading the original log file line by line as per the subtask instruction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "458bf43b"
      },
      "source": [
        "# Read the original HDFS log file line by line\n",
        "original_logs = []\n",
        "with open(hdfs_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        original_logs.append(line.strip())\n",
        "\n",
        "# Display the first few lines of the original logs\n",
        "print(\"First 5 lines of the original HDFS log file:\")\n",
        "for i in range(min(5, len(original_logs))):\n",
        "    print(original_logs[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81b90b7c"
      },
      "source": [
        "## Load the processed csv\n",
        "\n",
        "### Subtask:\n",
        "Load the `clean_hdfs.csv` file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a36a2cbd"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the `clean_hdfs.csv` file into a pandas DataFrame and display its head and info to verify.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "927a33ae"
      },
      "source": [
        "clean_hdfs_df = pd.read_csv('/content/clean_hdfs.csv')\n",
        "print(\"First 5 rows of clean_hdfs.csv:\")\n",
        "display(clean_hdfs_df.head())\n",
        "print(\"\\nInfo of clean_hdfs.csv:\")\n",
        "display(clean_hdfs_df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fc0e92a"
      },
      "source": [
        "## Compare data structures\n",
        "\n",
        "### Subtask:\n",
        "Describe the differences in structure between the raw log data and the structured DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58204d6c"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the structure of the raw log data, the processed DataFrame, and highlight the differences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6ff9977"
      },
      "source": [
        "# Describe the raw log data\n",
        "print(\"Raw log data format:\")\n",
        "print(f\"The raw log data is stored in a list called 'original_logs'.\")\n",
        "print(f\"It contains {len(original_logs)} elements, where each element is a string representing a single line from the original log file.\")\n",
        "print(\"Example of a raw log line:\")\n",
        "print(original_logs[0]) # Display the first line as an example\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Describe the processed DataFrame\n",
        "print(\"Processed DataFrame format:\")\n",
        "print(f\"The processed data is stored in a pandas DataFrame called 'clean_hdfs_df'.\")\n",
        "print(f\"The DataFrame has {clean_hdfs_df.shape[0]} rows and {clean_hdfs_df.shape[1]} columns.\")\n",
        "print(\"The columns are: \", clean_hdfs_df.columns.tolist())\n",
        "print(\"Example of the processed DataFrame structure:\")\n",
        "display(clean_hdfs_df.head())\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Highlight key structural differences\n",
        "print(\"Key structural differences:\")\n",
        "print(\"- The raw log data is unstructured text, a sequence of lines.\")\n",
        "print(\"- The processed data is structured in a tabular format (DataFrame) with named columns.\")\n",
        "print(\"- Each row in the DataFrame represents a summary of a log template, not an individual log line.\")\n",
        "print(\"- The DataFrame extracts specific information like 'template_id', 'template', and 'occurrences' from the log patterns.\")\n",
        "print(\"- The DataFrame includes additional computed features like 'template_id_scaled'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "673126ec"
      },
      "source": [
        "## Describe transformations\n",
        "\n",
        "### Subtask:\n",
        "Explain the steps taken in the code to transform the raw log data into the structured DataFrame, highlighting how Drain3 was used to extract templates and count occurrences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51c76af1"
      },
      "source": [
        "**Reasoning**:\n",
        "Describe the process of transforming raw log data into a structured DataFrame using Drain3 based on the provided instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60f19c0b"
      },
      "source": [
        "print(\"Explanation of the transformation from raw logs to structured DataFrame:\")\n",
        "\n",
        "# 1. Describe the initial state of the data\n",
        "print(\"\\n1. Initial State: Raw Log Data\")\n",
        "print(\"The initial data is the raw HDFS log file, which consists of unstructured text lines.\")\n",
        "print(\"Each line is a separate log entry, containing information like timestamp, process ID, log level, component, and the log message itself.\")\n",
        "print(\"Example raw line:\")\n",
        "if original_logs:\n",
        "    print(original_logs[0])\n",
        "else:\n",
        "    print(\"Original logs not available.\")\n",
        "\n",
        "# 2. Explain the purpose of Drain3\n",
        "print(\"\\n2. Purpose of Drain3\")\n",
        "print(\"The Drain3 library is used for log parsing and template extraction.\")\n",
        "print(\"Its main purpose is to group similar log messages together and identify a common template or pattern that represents the structure of those messages.\")\n",
        "print(\"This reduces the volume of data and allows for analysis based on log event types rather than individual log lines.\")\n",
        "\n",
        "# 3. Detail how TemplateMiner is initialized and used\n",
        "print(\"\\n3. Drain3 TemplateMiner Initialization and Usage\")\n",
        "print(\"A `TemplateMiner` instance is initialized with a persistence layer (`FilePersistence`) and configuration.\")\n",
        "print(\"The code then iterates through each line of the raw log file.\")\n",
        "print(\"For each `line.strip()`, the `template_miner.add_log_message()` method is called.\")\n",
        "print(\"Drain3 processes each message, comparing it against existing templates and either adding it to a matching cluster or creating a new cluster and template if no match is found.\")\n",
        "\n",
        "# 4. Explain how Drain3 groups and generates templates\n",
        "print(\"\\n4. Log Grouping and Template Generation\")\n",
        "print(\"As Drain3 processes log messages, it uses an algorithm (like the Drain algorithm) to identify variable parts of the log messages (like timestamps, IDs, file paths) and replace them with wildcards (`<*>`).\")\n",
        "print(\"Log messages that have the same structure after replacing variables are grouped into the same cluster.\")\n",
        "print(\"Each cluster is assigned a unique `template_id` and has a representative `template` string.\")\n",
        "\n",
        "# 5. Describe how the code iterates through clusters\n",
        "print(\"\\n5. Iterating through Drain3 Clusters\")\n",
        "print(\"After processing all log lines, the `template_miner.drain.clusters` attribute contains a list of all identified log clusters.\")\n",
        "print(\"The code iterates through this list of `cluster` objects.\")\n",
        "\n",
        "# 6. Explain extraction of template_id, template, and occurrences\n",
        "print(\"\\n6. Extracting Cluster Information\")\n",
        "print(\"For each `cluster` in the list, the code extracts:\")\n",
        "print(\"- `cluster.cluster_id`: The unique ID assigned to the template/cluster.\")\n",
        "print(\"- `cluster.get_template()`: The representative template string for the cluster.\")\n",
        "print(\"- `cluster.size`: The number of original log messages that belong to this cluster (occurrences).\")\n",
        "print(\"This information is stored as a dictionary with keys 'template_id', 'template', and 'occurrences'.\")\n",
        "print(\"These dictionaries are collected in a list called `logs`.\")\n",
        "\n",
        "# 7. Describe conversion to DataFrame\n",
        "print(\"\\n7. Conversion to Pandas DataFrame\")\n",
        "print(\"Finally, the list of dictionaries (`logs`) is converted into a pandas DataFrame using `pd.DataFrame(logs)`.\")\n",
        "print(\"Each dictionary in the list becomes a row in the DataFrame.\")\n",
        "print(\"The keys of the dictionaries ('template_id', 'template', 'occurrences') become the column names.\")\n",
        "print(\"This results in the `hdfs_df` DataFrame, where each row summarizes a unique log template found in the original data, along with its frequency.\")\n",
        "display(hdfs_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ab7d94"
      },
      "source": [
        "## Compare content\n",
        "\n",
        "### Subtask:\n",
        "Illustrate the mapping between original log lines and the corresponding entries in the `clean_hdfs.csv`, showing examples of how templates were generated and how occurrences were counted.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8971376"
      },
      "source": [
        "**Reasoning**:\n",
        "Select a few templates and find corresponding original log lines to illustrate the mapping and explain the process of template generation and occurrence counting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "762eb940"
      },
      "source": [
        "import re\n",
        "\n",
        "# Select a few templates with varying occurrences\n",
        "selected_templates = hdfs_df.sort_values('occurrences', ascending=False).head(3)\n",
        "print(\"Selected Templates for Illustration:\")\n",
        "display(selected_templates[['template_id', 'template', 'occurrences']])\n",
        "\n",
        "# Find and show original log lines for each selected template\n",
        "print(\"\\nIllustration of Template Generation and Occurrence Counting:\")\n",
        "for index, row in selected_templates.iterrows():\n",
        "    template_id = row['template_id']\n",
        "    template_str = row['template']\n",
        "    occurrences = row['occurrences']\n",
        "\n",
        "    print(f\"\\n--- Template ID: {template_id} ---\")\n",
        "    print(f\"Template: {template_str}\")\n",
        "    print(f\"Occurrences in DataFrame: {occurrences}\")\n",
        "\n",
        "    # Escape special characters for regex, but keep the wildcard <*>\n",
        "    regex_pattern = re.escape(template_str).replace(re.escape('<*>'), '.*')\n",
        "\n",
        "    # Find original log lines matching the template (show up to 3 examples)\n",
        "    matching_logs = [line for line in original_logs if re.fullmatch(regex_pattern, line)]\n",
        "\n",
        "    print(f\"Examples of Original Log Lines Matching this Template ({min(3, len(matching_logs))} shown):\")\n",
        "    for i in range(min(3, len(matching_logs))):\n",
        "        print(f\"- {matching_logs[i]}\")\n",
        "\n",
        "    # Explanation of template generation\n",
        "    print(\"\\nExplanation:\")\n",
        "    print(f\"Drain3 generated this template by identifying the common structure in {occurrences} original log lines.\")\n",
        "    print(\"Variable parts of the log messages (like timestamps, process IDs, block IDs, IP addresses, file paths, etc.) were replaced by the wildcard '<*>'.\")\n",
        "    print(\"For example, in the original lines shown above, the parts that differ are replaced by '<*>' in the template.\")\n",
        "\n",
        "    # Explanation of occurrence counting\n",
        "    print(f\"The 'occurrences' count ({occurrences}) in the DataFrame for this template represents the total number of original log lines that were found to match this specific template by Drain3.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4c8982"
      },
      "source": [
        "## Highlight added columns\n",
        "\n",
        "### Subtask:\n",
        "Explain the purpose of the `label`, `template_id_scaled`, and `anomaly` columns added to the DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d68b937"
      },
      "source": [
        "**Reasoning**:\n",
        "Provide an explanation for the purpose of the `label`, `template_id_scaled`, and `anomaly` columns, following the specified instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75e6e219"
      },
      "source": [
        "print(\"Explanation of additional columns in the DataFrame (`label`, `template_id_scaled`, `anomaly`):\")\n",
        "\n",
        "# Explain 'label' column\n",
        "print(\"\\n- 'label' column:\")\n",
        "print(\"  This column was added as a placeholder (currently filled with 0s) to represent ground truth labels.\")\n",
        "print(\"  In a real-world anomaly detection scenario with labeled data, this column would contain the true status of each log template (e.g., 0 for normal, 1 for anomaly), which could be used to train or evaluate a supervised learning model.\")\n",
        "print(\"  For this demonstration using unsupervised anomaly detection (Isolation Forest), the 'label' column is not used in the anomaly detection process itself but is included to show where ground truth labels would typically fit.\")\n",
        "\n",
        "# Explain 'template_id_scaled' column\n",
        "print(\"\\n- 'template_id_scaled' column:\")\n",
        "print(\"  This column contains a scaled version of the 'template_id' column using StandardScaler.\")\n",
        "print(\"  Scaling is a common preprocessing step in machine learning.\")\n",
        "print(\"  Many algorithms, including distance-based or gradient-based methods (though Isolation Forest is not strictly one of these, it can still benefit from features on a similar scale), perform better when numerical input features are scaled to have a mean of 0 and a standard deviation of 1.\")\n",
        "print(\"  Scaling the 'template_id' puts it on a comparable scale to other numerical features like 'occurrences', which is important when using both features in the Isolation Forest model.\")\n",
        "\n",
        "# Explain 'anomaly' column\n",
        "print(\"\\n- 'anomaly' column:\")\n",
        "print(\"  This column contains the output of the Isolation Forest anomaly detection model.\")\n",
        "print(\"  The Isolation Forest model was trained on the 'occurrences' and 'template_id_scaled' features.\")\n",
        "print(\"  The model predicts whether each log template (represented by a row) is an outlier or anomaly.\")\n",
        "print(\"  A value of 1 in this column indicates that the Isolation Forest model classified the log template as anomalous (an outlier).\")\n",
        "print(\"  A value of 0 indicates that the model classified it as normal (inlier).\")\n",
        "\n",
        "print(\"\\nThese columns were added *after* the initial log parsing and structuring steps performed by Drain3.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1c48dde"
      },
      "source": [
        "## Summarize changes\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the key changes made to the data, from the raw log file to the final DataFrame with anomaly labels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dca184bf"
      },
      "source": [
        "**Reasoning**:\n",
        "Provide a summary of the key changes made to the data, from the raw log file to the final DataFrame with anomaly labels, following the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7870bd74"
      },
      "source": [
        "print(\"Summary of Data Transformation from Raw Logs to Final DataFrame:\")\n",
        "\n",
        "# 1. Summarize the initial state\n",
        "print(\"\\n1. Initial State: Raw HDFS Log File\")\n",
        "print(\"The process starts with the raw HDFS log file, which is a collection of unstructured text lines.\")\n",
        "print(\"Each line is a single log entry, containing varying information like timestamps, process IDs, components, and the log message content in a free-text format.\")\n",
        "print(\"This format is difficult to analyze directly for patterns and anomalies.\")\n",
        "\n",
        "# 2. Describe the Drain3 transformation\n",
        "print(\"\\n2. Log Parsing and Structuring with Drain3\")\n",
        "print(\"The Drain3 log parsing library is applied to the raw log data.\")\n",
        "print(\"Drain3 identifies recurring patterns (templates) within the log messages and groups similar log lines together into clusters.\")\n",
        "print(\"This step transforms the unstructured text into a structured format where each unique log template is identified.\")\n",
        "print(\"The output of this stage, initially, is information about each template: a unique `template_id`, the generalized `template` string (with wildcards like '<*>'), and the `occurrences` count, representing how many raw log lines matched that template.\")\n",
        "print(\"This structured information is then organized into a pandas DataFrame.\")\n",
        "\n",
        "# 3. Explain the addition of the 'label' column\n",
        "print(\"\\n3. Addition of 'label' Column\")\n",
        "print(\"A 'label' column is added to the DataFrame.\")\n",
        "print(\"In this specific case (unsupervised anomaly detection), it is initialized with a default value (0, representing 'normal') as a placeholder.\")\n",
        "print(\"In a supervised learning scenario, this column would contain actual ground truth labels indicating whether a log template is known to be normal or anomalous.\")\n",
        "\n",
        "# 4. Summarize the creation of 'template_id_scaled'\n",
        "print(\"\\n4. Creation of 'template_id_scaled'\")\n",
        "print(\"The 'template_id' column, which is a discrete numerical identifier, is scaled using StandardScaler.\")\n",
        "print(\"This creates the 'template_id_scaled' column, which has a mean of 0 and a standard deviation of 1.\")\n",
        "print(\"Scaling is performed to prepare this feature for use in machine learning models, ensuring it is on a comparable scale to other numerical features like 'occurrences'.\")\n",
        "\n",
        "# 5. Describe the addition of the 'anomaly' column\n",
        "print(\"\\n5. Anomaly Detection and Addition of 'anomaly' Column\")\n",
        "print(\"The Isolation Forest model, an unsupervised anomaly detection algorithm, is applied.\")\n",
        "print(\"The model uses the 'occurrences' and 'template_id_scaled' features to identify outliers among the log templates.\")\n",
        "print(\"Based on the model's predictions, an 'anomaly' column is added to the DataFrame.\")\n",
        "print(\"A value of 1 in this column indicates that the model identified the corresponding log template as anomalous (an outlier), while a value of 0 indicates it was identified as normal (an inlier).\")\n",
        "\n",
        "# 6. Conclude on the overall transformation\n",
        "print(\"\\nOverall Transformation:\")\n",
        "print(\"The entire process transforms raw, unstructured HDFS log text into a structured DataFrame where each row represents a log template.\")\n",
        "print(\"This DataFrame contains the template details, occurrence counts, a placeholder for ground truth labels, a scaled feature for machine learning, and importantly, a binary flag indicating whether the template was classified as an anomaly by the Isolation Forest model.\")\n",
        "print(\"This structured and enriched format is suitable for further analysis, visualization, and understanding of log patterns and anomalies.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f73dd771"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The original HDFS log data is in an unstructured text format, consisting of a sequence of individual log lines (e.g., `20151007 215958 672 INFO FSNamesystem: BLOCK* NameSystem.allocateBlock: /hdfs/kX7j62M10 <*><*> 10.250.14.12:50010`).\n",
        "*   The processed data is structured into a pandas DataFrame (`clean_hdfs.csv`) with 18 rows and 5 columns: `template_id`, `template`, `occurrences`, `label`, and `template_id_scaled`.\n",
        "*   The transformation from raw logs to the DataFrame was primarily performed by the Drain3 library, which parsed the log lines, grouped similar messages, and extracted a generalized `template` for each group.\n",
        "*   The `template_id` column is a unique numerical identifier assigned to each log template by Drain3.\n",
        "*   The `template` column contains the generalized pattern of the log messages in a group, with variable parts replaced by the wildcard `<*>`.\n",
        "*   The `occurrences` column represents the count of original log lines that matched a specific template identified by Drain3.\n",
        "*   The `label` column was added as a placeholder for ground truth labels (currently all 0s) for potential future supervised learning.\n",
        "*   The `template_id_scaled` column contains a scaled version of `template_id` using StandardScaler, prepared for machine learning algorithms.\n",
        "*   An `anomaly` column was added based on the output of an Isolation Forest model trained on `occurrences` and `template_id_scaled`, indicating whether a log template was classified as anomalous (1) or normal (0).\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The transformation process effectively converts unstructured log text into a structured format based on recurring patterns, significantly reducing the volume of data and making it suitable for quantitative analysis.\n",
        "*   The resulting DataFrame is prepared for anomaly detection, with key features (`occurrences`, `template_id_scaled`) and an anomaly prediction (`anomaly`) column. The next steps could involve analyzing the detected anomalies, visualizing the results, or using the `label` column (if ground truth data becomes available) to evaluate the anomaly detection model.\n"
      ]
    }
  ]
}